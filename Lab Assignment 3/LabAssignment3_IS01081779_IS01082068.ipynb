{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5bb8ce3b",
   "metadata": {},
   "source": [
    "# Group Members: \n",
    "# 1. Ayman Fikry bin Asmajuda (IS01081779)\n",
    "# 2. Muhammad Khairin Asnawi bin Rosli (IS01082068) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca66807",
   "metadata": {},
   "source": [
    "# 1.0 Read the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c164c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a0ef41d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>title</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>I was wondering if anyone out there could enli...</td>\n",
       "      <td>7</td>\n",
       "      <td>rec.autos</td>\n",
       "      <td>2022-08-02 13:48:37.251043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17</td>\n",
       "      <td>I recently posted an article asking what kind ...</td>\n",
       "      <td>7</td>\n",
       "      <td>rec.autos</td>\n",
       "      <td>2022-08-02 13:48:37.251043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>29</td>\n",
       "      <td>\\nIt depends on your priorities.  A lot of peo...</td>\n",
       "      <td>7</td>\n",
       "      <td>rec.autos</td>\n",
       "      <td>2022-08-02 13:48:37.251043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56</td>\n",
       "      <td>an excellent automatic can be found in the sub...</td>\n",
       "      <td>7</td>\n",
       "      <td>rec.autos</td>\n",
       "      <td>2022-08-02 13:48:37.251043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>64</td>\n",
       "      <td>: Ford and his automobile.  I need information...</td>\n",
       "      <td>7</td>\n",
       "      <td>rec.autos</td>\n",
       "      <td>2022-08-02 13:48:37.251043</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                               text  target  \\\n",
       "0           0  I was wondering if anyone out there could enli...       7   \n",
       "1          17  I recently posted an article asking what kind ...       7   \n",
       "2          29  \\nIt depends on your priorities.  A lot of peo...       7   \n",
       "3          56  an excellent automatic can be found in the sub...       7   \n",
       "4          64  : Ford and his automobile.  I need information...       7   \n",
       "\n",
       "       title                        date  \n",
       "0  rec.autos  2022-08-02 13:48:37.251043  \n",
       "1  rec.autos  2022-08-02 13:48:37.251043  \n",
       "2  rec.autos  2022-08-02 13:48:37.251043  \n",
       "3  rec.autos  2022-08-02 13:48:37.251043  \n",
       "4  rec.autos  2022-08-02 13:48:37.251043  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset\n",
    "data = pd.read_csv('news_dataset.csv', nrows=1000)\n",
    "\n",
    "# Preview the head of the data\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ccc034c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select Score and Text column only\n",
    "data = data[['text']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6934bb",
   "metadata": {},
   "source": [
    "# 2.0 Perform Text Pre-processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1feb9a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41\n"
     ]
    }
   ],
   "source": [
    "# Find the number of duplicated rows\n",
    "duplicate_text = data.duplicated()\n",
    "print(duplicate_text.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa62dbb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(959, 1)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove the duplicated rows except the first one\n",
    "data = data.drop_duplicates(keep='first')\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc5da9a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  text\n",
      "0    I was wondering if anyone out there could enli...\n",
      "1    I recently posted an article asking what kind ...\n",
      "2    \\nIt depends on your priorities.  A lot of peo...\n",
      "3    an excellent automatic can be found in the sub...\n",
      "4    : Ford and his automobile.  I need information...\n",
      "..                                                 ...\n",
      "995  Hi there,\\n\\nwhen I run Disk First Aid on my e...\n",
      "996  \\nI agree completely, but there was only a ref...\n",
      "997  \\n\\n\\n\\n\\n\\n\\nAlso, has anyone heard any rumor...\n",
      "998  \\n\\nSince I repost this message again for the ...\n",
      "999  Hi!\\n\\nI am looking for ftp sites (where there...\n",
      "\n",
      "[959 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "# Check any rows available\n",
    "print(data) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528dbf2f",
   "metadata": {},
   "source": [
    "## 2.1 Cleaning & Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e63dd1ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i was wondering if anyone out there could enli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i recently posted an article asking what kind ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>it depends on your priorities a lot of people ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>an excellent automatic can be found in the sub...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ford and his automobile i need information on ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  i was wondering if anyone out there could enli...\n",
       "1  i recently posted an article asking what kind ...\n",
       "2  it depends on your priorities a lot of people ...\n",
       "3  an excellent automatic can be found in the sub...\n",
       "4  ford and his automobile i need information on ..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)\n",
    "\n",
    "    # Remove HTML tags\n",
    "    cleaned_text = re.sub(r'<.*?>', '', text)\n",
    "    \n",
    "    # Remove URLs\n",
    "    cleaned_text = re.sub(r'http\\S+', '', cleaned_text)\n",
    "    \n",
    "    # Remove special characters and digits\n",
    "    cleaned_text = re.sub(r'[^a-zA-Z\\s]', '', cleaned_text)\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    cleaned_text = cleaned_text.lower()\n",
    "    \n",
    "    # Remove extra whitespaces\n",
    "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text)\n",
    "    \n",
    "    return cleaned_text.strip()\n",
    "\n",
    "# Apply the clean_text function to the 'Text' column\n",
    "data['text'] = data['text'].apply(clean_text)\n",
    "\n",
    "# View the cleaned text data\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a73012c",
   "metadata": {},
   "source": [
    "## 2.2 Remove null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b0889f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for missing values\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a009d625",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select relevant columns for sentiment analysis\n",
    "data = data[['text']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ceb992d",
   "metadata": {},
   "source": [
    "## 2.3 Tokenization & Stopword Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7499d942",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ayman\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ayman\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def tokenize_text(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [token for token in tokens if token.lower() not in stop_words]\n",
    "    return filtered_tokens\n",
    "\n",
    "# Tokenize the text into individual words\n",
    "data['Tokens'] = data['text'].apply(tokenize_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696f94c7",
   "metadata": {},
   "source": [
    "## 2.4 Lemmatization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0367a3b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ayman\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download the WordNet lemmatizer\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "20449d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the WordNet lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Lemmatize the tokens\n",
    "data['Tokens'] = data['Tokens'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "df951da7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  text  \\\n",
      "0    i was wondering if anyone out there could enli...   \n",
      "1    i recently posted an article asking what kind ...   \n",
      "2    it depends on your priorities a lot of people ...   \n",
      "3    an excellent automatic can be found in the sub...   \n",
      "4    ford and his automobile i need information on ...   \n",
      "..                                                 ...   \n",
      "995  hi there when i run disk first aid on my exter...   \n",
      "996  i agree completely but there was only a refund...   \n",
      "997  also has anyone heard any rumors that the new ...   \n",
      "998  since i repost this message again for the seco...   \n",
      "999  hi i am looking for ftp sites where there are ...   \n",
      "\n",
      "                                                Tokens  \\\n",
      "0    [wondering, anyone, could, enlighten, car, saw...   \n",
      "1    [recently, posted, article, asking, kind, rate...   \n",
      "2    [depends, priority, lot, people, put, higher, ...   \n",
      "3    [excellent, automatic, found, subaru, legacy, ...   \n",
      "4    [ford, automobile, need, information, whether,...   \n",
      "..                                                 ...   \n",
      "995  [hi, run, disk, first, aid, external, hard, dr...   \n",
      "996  [agree, completely, refund, people, bought, gc...   \n",
      "997  [also, anyone, heard, rumor, new, dock, one, c...   \n",
      "998  [since, repost, message, second, time, hope, h...   \n",
      "999  [hi, looking, ftp, site, freeware, shareware, ...   \n",
      "\n",
      "                                     Preprocessed_Text  \n",
      "0    wondering anyone could enlighten car saw day d...  \n",
      "1    recently posted article asking kind rate singl...  \n",
      "2    depends priority lot people put higher priorit...  \n",
      "3    excellent automatic found subaru legacy switch...  \n",
      "4    ford automobile need information whether ford ...  \n",
      "..                                                 ...  \n",
      "995  hi run disk first aid external hard drive quan...  \n",
      "996  agree completely refund people bought gc quadr...  \n",
      "997  also anyone heard rumor new dock one cpu bette...  \n",
      "998  since repost message second time hope hear fol...  \n",
      "999  hi looking ftp site freeware shareware mac hel...  \n",
      "\n",
      "[959 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# Join the tokens back into sentences\n",
    "data['Preprocessed_Text'] = data['Tokens'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "# Save the preprocessed data to a new CSV file\n",
    "data.to_csv('processed.csv', index=False)\n",
    "\n",
    "# Preview the preprocessed data\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e727a1",
   "metadata": {},
   "source": [
    "# 3.0 Perform LDA using Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "16c40e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For topic modeling\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3e285103",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.011*\"mac\" + 0.007*\"bit\" + 0.007*\"apple\" + 0.007*\"mb\" + 0.007*\"card\"')\n",
      "(1, '0.011*\"car\" + 0.007*\"oil\" + 0.006*\"problem\" + 0.006*\"get\" + 0.006*\"would\"')\n",
      "(2, '0.016*\"car\" + 0.010*\"would\" + 0.009*\"like\" + 0.008*\"one\" + 0.006*\"good\"')\n",
      "(3, '0.019*\"car\" + 0.005*\"would\" + 0.005*\"one\" + 0.005*\"get\" + 0.005*\"year\"')\n",
      "(4, '0.008*\"drive\" + 0.008*\"car\" + 0.005*\"get\" + 0.005*\"know\" + 0.005*\"send\"')\n"
     ]
    }
   ],
   "source": [
    "# Split the preprocessed text into a list of lists of tokens\n",
    "preprocessed_documents = [doc.split() for doc in data['Preprocessed_Text']]\n",
    "\n",
    "# Create a Gensim Dictionary object from the preprocessed documents\n",
    "dictionary = corpora.Dictionary(preprocessed_documents)\n",
    "\n",
    "# Convert each preprocessed document into a bag-of-words representation using the dictionary\n",
    "corpus = [dictionary.doc2bow(doc) for doc in preprocessed_documents]\n",
    "\n",
    "# Train an LDA model on the corpus\n",
    "lda_model = LdaModel(corpus, num_topics=5, id2word=dictionary, passes=10)\n",
    "\n",
    "# Display the topics found by the LDA model\n",
    "topics = lda_model.print_topics(num_words=5)\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ffa31da4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table with Articles and Topic:\n",
      "                                               Article  Topic\n",
      "0    wondering anyone could enlighten car saw day d...      2\n",
      "1    recently posted article asking kind rate singl...      3\n",
      "2    depends priority lot people put higher priorit...      2\n",
      "3    excellent automatic found subaru legacy switch...      2\n",
      "4    ford automobile need information whether ford ...      3\n",
      "..                                                 ...    ...\n",
      "995  hi run disk first aid external hard drive quan...      2\n",
      "996  agree completely refund people bought gc quadr...      0\n",
      "997  also anyone heard rumor new dock one cpu bette...      0\n",
      "998  since repost message second time hope hear fol...      4\n",
      "999  hi looking ftp site freeware shareware mac hel...      0\n",
      "\n",
      "[959 rows x 2 columns]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# empty list to store dominant topic labels for each document\n",
    "article_labels = []\n",
    "\n",
    "# iterate over each processed document\n",
    "for i, doc in enumerate(preprocessed_documents):\n",
    "    # for each document, convert to bag-of-words representation\n",
    "    bow = dictionary.doc2bow(doc)\n",
    "    # get list of topic probabilities\n",
    "    topics = lda_model.get_document_topics(bow)\n",
    "    # determine topic with highest probability\n",
    "    dominant_topic = max(topics, key=lambda x: x[1])[0]\n",
    "    # append to the list\n",
    "    article_labels.append(dominant_topic)\n",
    "\n",
    "# Create DataFrame\n",
    "df_result = pd.DataFrame({\"Article\": data['Preprocessed_Text'], \"Topic\": article_labels})\n",
    "\n",
    "# Print the DataFrame\n",
    "print(\"Table with Articles and Topic:\")\n",
    "print(df_result)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8a0cc15d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Terms for Each Topic:\n",
      "Topic 0:\n",
      "- \"mac\" (weight: 0.011)\n",
      "- \"bit\" (weight: 0.007)\n",
      "- \"apple\" (weight: 0.007)\n",
      "- \"mb\" (weight: 0.007)\n",
      "- \"card\" (weight: 0.007)\n",
      "- \"one\" (weight: 0.006)\n",
      "- \"use\" (weight: 0.005)\n",
      "- \"system\" (weight: 0.005)\n",
      "- \"problem\" (weight: 0.005)\n",
      "- \"machine\" (weight: 0.005)\n",
      "\n",
      "Topic 1:\n",
      "- \"car\" (weight: 0.011)\n",
      "- \"oil\" (weight: 0.007)\n",
      "- \"problem\" (weight: 0.006)\n",
      "- \"get\" (weight: 0.006)\n",
      "- \"would\" (weight: 0.006)\n",
      "- \"tire\" (weight: 0.005)\n",
      "- \"brake\" (weight: 0.004)\n",
      "- \"one\" (weight: 0.004)\n",
      "- \"engine\" (weight: 0.004)\n",
      "- \"dont\" (weight: 0.003)\n",
      "\n",
      "Topic 2:\n",
      "- \"car\" (weight: 0.016)\n",
      "- \"would\" (weight: 0.010)\n",
      "- \"like\" (weight: 0.009)\n",
      "- \"one\" (weight: 0.008)\n",
      "- \"good\" (weight: 0.006)\n",
      "- \"dont\" (weight: 0.005)\n",
      "- \"get\" (weight: 0.005)\n",
      "- \"time\" (weight: 0.005)\n",
      "- \"speed\" (weight: 0.004)\n",
      "- \"new\" (weight: 0.004)\n",
      "\n",
      "Topic 3:\n",
      "- \"car\" (weight: 0.019)\n",
      "- \"would\" (weight: 0.005)\n",
      "- \"one\" (weight: 0.005)\n",
      "- \"get\" (weight: 0.005)\n",
      "- \"year\" (weight: 0.005)\n",
      "- \"dealer\" (weight: 0.004)\n",
      "- \"also\" (weight: 0.004)\n",
      "- \"much\" (weight: 0.003)\n",
      "- \"ive\" (weight: 0.003)\n",
      "- \"engine\" (weight: 0.003)\n",
      "\n",
      "Topic 4:\n",
      "- \"drive\" (weight: 0.008)\n",
      "- \"car\" (weight: 0.008)\n",
      "- \"get\" (weight: 0.005)\n",
      "- \"know\" (weight: 0.005)\n",
      "- \"send\" (weight: 0.005)\n",
      "- \"new\" (weight: 0.005)\n",
      "- \"list\" (weight: 0.004)\n",
      "- \"problem\" (weight: 0.004)\n",
      "- \"please\" (weight: 0.004)\n",
      "- \"request\" (weight: 0.004)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the top terms for each topic\n",
    "print(\"Top Terms for Each Topic:\")\n",
    "for idx, topic in lda_model.print_topics():\n",
    "    print(f\"Topic {idx}:\")\n",
    "    terms = [term.strip() for term in topic.split(\"+\")]\n",
    "    for term in terms:\n",
    "        weight, word = term.split(\"*\")\n",
    "        print(f\"- {word.strip()} (weight: {weight.strip()})\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a2fdd7",
   "metadata": {},
   "source": [
    "# 4.0 Evaluate LDA using Coherence Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "35b9532e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import library for Coherence Score\n",
    "\n",
    "from gensim.models.coherencemodel import CoherenceModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "21da9a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the coherence score for the LDA model\n",
    "\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=preprocessed_documents, dictionary=dictionary, coherence='c_v')\n",
    "\n",
    "coherence_lda = coherence_model_lda.get_coherence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5df57b54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic Coherence Score (C_V): 0.3560\n"
     ]
    }
   ],
   "source": [
    "# Display the score\n",
    "\n",
    "print(f'Topic Coherence Score (C_V): {coherence_lda:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef71693b",
   "metadata": {},
   "source": [
    "# 5.0 Interpret the result\n",
    "### First and foremost, topic 0 appears to be more focus on general conversations on computers, where the weight of terms like \"mac\" seems to be the highest which indicate the importance of the term for this topic.\n",
    "### In addition to that, topic 1, 2 and 3 emphasize towards vehicles, where the weight of terms like \"car\" are relatively high, which suggest a strong association for the three topics.\n",
    "### The final topic, topic 4, seems to be related around vehicles and driving, where the weight of terms like \"drive\" and \"car\" are particularly high, indicating their significance in this topic.\n",
    "### There is an obvious repitition of terms as the term \"car\" appears often in practically most of the topics, suggesting that the corpus is mostly concerned with conversations concerning vehicles particularly cars. This might imply that the dataset mostly relates to automobiles and associated problems.\n",
    "### On a similar note, Topic 0 is unique in that it appears to be concentrated on technology and Apple products. This suggests that a portion of the conversations on a separate topic may be included in the dataset.\n",
    "### The results shows a topic coherence score of 0.356 which indicates a moderate level of coherence. This implies that although the topics determined by the LDA model make sense in certain ways, there are still some improvements or enhancements to be made. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
