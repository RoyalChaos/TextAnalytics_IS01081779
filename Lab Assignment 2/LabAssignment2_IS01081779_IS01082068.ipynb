{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c164c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a0ef41d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>B001E4KFG0</td>\n",
       "      <td>A3SGXH7AUHU8GW</td>\n",
       "      <td>delmartian</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1303862400</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>B00813GRG4</td>\n",
       "      <td>A1D87F6ZCVE5NK</td>\n",
       "      <td>dll pa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1346976000</td>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>B000LQOCH0</td>\n",
       "      <td>ABXLMWJIXXAIN</td>\n",
       "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1219017600</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>B000UA0QIQ</td>\n",
       "      <td>A395BORC6FGVXV</td>\n",
       "      <td>Karl</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1307923200</td>\n",
       "      <td>Cough Medicine</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>A1UQRSCLF8GW1T</td>\n",
       "      <td>Michael D. Bigham \"M. Wassir\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1350777600</td>\n",
       "      <td>Great taffy</td>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id   ProductId          UserId                      ProfileName  \\\n",
       "0   1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n",
       "1   2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
       "2   3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
       "3   4  B000UA0QIQ  A395BORC6FGVXV                             Karl   \n",
       "4   5  B006K2ZZ7K  A1UQRSCLF8GW1T    Michael D. Bigham \"M. Wassir\"   \n",
       "\n",
       "   HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "0                     1                       1      5  1303862400   \n",
       "1                     0                       0      1  1346976000   \n",
       "2                     1                       1      4  1219017600   \n",
       "3                     3                       3      2  1307923200   \n",
       "4                     0                       0      5  1350777600   \n",
       "\n",
       "                 Summary                                               Text  \n",
       "0  Good Quality Dog Food  I have bought several of the Vitality canned d...  \n",
       "1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...  \n",
       "2  \"Delight\" says it all  This is a confection that has been around a fe...  \n",
       "3         Cough Medicine  If you are looking for the secret ingredient i...  \n",
       "4            Great taffy  Great taffy at a great price.  There was a wid...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset\n",
    "data = pd.read_csv('Reviews.csv', nrows=1000)\n",
    "\n",
    "# Preview the head of the data\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6934bb",
   "metadata": {},
   "source": [
    "# 1.0 Data Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ccc034c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select Score and Text column only\n",
    "data = data[['Score', 'Text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1feb9a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "# Find the number of duplicated rows\n",
    "duplicate_text = data.duplicated()\n",
    "print(duplicate_text.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa62dbb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(997, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove the duplicated rows except the first one\n",
    "data = data.drop_duplicates(keep='first')\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc5da9a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Score                                               Text\n",
      "0        5  I have bought several of the Vitality canned d...\n",
      "1        1  Product arrived labeled as Jumbo Salted Peanut...\n",
      "2        4  This is a confection that has been around a fe...\n",
      "3        2  If you are looking for the secret ingredient i...\n",
      "4        5  Great taffy at a great price.  There was a wid...\n",
      "..     ...                                                ...\n",
      "995      5  BLACK MARKET HOT SAUCE IS WONDERFUL.... My hus...\n",
      "996      5  Man what can i say, this salsa is the bomb!! i...\n",
      "997      5  this sauce is so good with just about anything...\n",
      "998      1  Not hot at all. Like the other low star review...\n",
      "999      2  I have to admit, I was a sucker for the large ...\n",
      "\n",
      "[997 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Check any rows available\n",
    "print(data) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528dbf2f",
   "metadata": {},
   "source": [
    "## 1.1 Cleaning & Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e63dd1ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Score</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>i have bought several of the vitality canned d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>product arrived labeled as jumbo salted peanut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>this is a confection that has been around a fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>if you are looking for the secret ingredient i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>great taffy at a great price there was a wide ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Score                                               Text\n",
       "0      5  i have bought several of the vitality canned d...\n",
       "1      1  product arrived labeled as jumbo salted peanut...\n",
       "2      4  this is a confection that has been around a fe...\n",
       "3      2  if you are looking for the secret ingredient i...\n",
       "4      5  great taffy at a great price there was a wide ..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    # Remove HTML tags\n",
    "    cleaned_text = re.sub(r'<.*?>', '', text)\n",
    "    \n",
    "    # Remove URLs\n",
    "    cleaned_text = re.sub(r'http\\S+', '', cleaned_text)\n",
    "    \n",
    "    # Remove special characters and digits\n",
    "    cleaned_text = re.sub(r'[^a-zA-Z\\s]', '', cleaned_text)\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    cleaned_text = cleaned_text.lower()\n",
    "    \n",
    "    # Remove extra whitespaces\n",
    "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text)\n",
    "    \n",
    "    return cleaned_text.strip()\n",
    "\n",
    "# Apply the clean_text function to the 'Text' column\n",
    "data['Text'] = data['Text'].apply(clean_text)\n",
    "\n",
    "# View the cleaned text data\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b0889f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Score    0\n",
       "Text     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for missing values\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a009d625",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select relevant columns for sentiment analysis\n",
    "data = data[['Score', 'Text']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ceb992d",
   "metadata": {},
   "source": [
    "## 1.2 Tokenization & Stopword Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7499d942",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ayman\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ayman\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def tokenize_text(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [token for token in tokens if token.lower() not in stop_words]\n",
    "    return filtered_tokens\n",
    "\n",
    "# Tokenize the text into individual words\n",
    "data['Tokens'] = data['Text'].apply(tokenize_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696f94c7",
   "metadata": {},
   "source": [
    "## 1.3 Lemmatization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0367a3b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ayman\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download the WordNet lemmatizer\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "20449d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the WordNet lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Lemmatize the tokens\n",
    "data['Tokens'] = data['Tokens'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "df951da7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Score                                               Text  \\\n",
      "0        5  i have bought several of the vitality canned d...   \n",
      "1        1  product arrived labeled as jumbo salted peanut...   \n",
      "2        4  this is a confection that has been around a fe...   \n",
      "3        2  if you are looking for the secret ingredient i...   \n",
      "4        5  great taffy at a great price there was a wide ...   \n",
      "..     ...                                                ...   \n",
      "995      5  black market hot sauce is wonderful my husband...   \n",
      "996      5  man what can i say this salsa is the bomb i ha...   \n",
      "997      5  this sauce is so good with just about anything...   \n",
      "998      1  not hot at all like the other low star reviewe...   \n",
      "999      2  i have to admit i was a sucker for the large q...   \n",
      "\n",
      "                                                Tokens  \\\n",
      "0    [bought, several, vitality, canned, dog, food,...   \n",
      "1    [product, arrived, labeled, jumbo, salted, pea...   \n",
      "2    [confection, around, century, light, pillowy, ...   \n",
      "3    [looking, secret, ingredient, robitussin, beli...   \n",
      "4    [great, taffy, great, price, wide, assortment,...   \n",
      "..                                                 ...   \n",
      "995  [black, market, hot, sauce, wonderful, husband...   \n",
      "996  [man, say, salsa, bomb, different, kind, almos...   \n",
      "997  [sauce, good, anything, like, adding, asian, f...   \n",
      "998  [hot, like, low, star, reviewer, got, suckered...   \n",
      "999  [admit, sucker, large, quantity, oz, shopping,...   \n",
      "\n",
      "                                     Preprocessed_Text  \n",
      "0    bought several vitality canned dog food produc...  \n",
      "1    product arrived labeled jumbo salted peanutsth...  \n",
      "2    confection around century light pillowy citrus...  \n",
      "3    looking secret ingredient robitussin believe f...  \n",
      "4    great taffy great price wide assortment yummy ...  \n",
      "..                                                 ...  \n",
      "995  black market hot sauce wonderful husband love ...  \n",
      "996  man say salsa bomb different kind almost every...  \n",
      "997  sauce good anything like adding asian food any...  \n",
      "998  hot like low star reviewer got suckered seeing...  \n",
      "999  admit sucker large quantity oz shopping hot sa...  \n",
      "\n",
      "[997 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# Join the tokens back into sentences\n",
    "data['Preprocessed_Text'] = data['Tokens'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "# Save the preprocessed data to a new CSV file\n",
    "data.to_csv('processed.csv', index=False)\n",
    "\n",
    "# Preview the preprocessed data\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878cc5a4",
   "metadata": {},
   "source": [
    "# 2.0 Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a82756",
   "metadata": {},
   "source": [
    "## 2.1 Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "23a2ae32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag of Words (BoW) feature shape: (997, 5937)\n",
      "Vocabulary size: 5937\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Initialize the CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit and transform the preprocessed text data\n",
    "bow_features = vectorizer.fit_transform(data['Preprocessed_Text'])\n",
    "\n",
    "# Get the vocabulary (unique words)\n",
    "vocabulary = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Print the shape of the BoW features and the vocabulary size\n",
    "print(\"Bag of Words (BoW) feature shape:\", bow_features.shape)\n",
    "print(\"Vocabulary size:\", len(vocabulary))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf763143",
   "metadata": {},
   "source": [
    "## 2.2 TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6d8222bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF feature shape: (997, 5937)\n",
      "Vocabulary size: 5937\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize the TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the preprocessed text data\n",
    "tfidf_features = tfidf_vectorizer.fit_transform(data['Preprocessed_Text'])\n",
    "\n",
    "# Get the vocabulary (unique words)\n",
    "tfidf_vocabulary = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "# Print the shape of the TF-IDF features and the vocabulary size\n",
    "print(\"TF-IDF feature shape:\", tfidf_features.shape)\n",
    "print(\"Vocabulary size:\", len(tfidf_vocabulary))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2120a0b3",
   "metadata": {},
   "source": [
    "# 3.0 Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6e73a8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcda05e4",
   "metadata": {},
   "source": [
    "## 3.1 Lexicon-based Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "045bfcf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\ayman\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the Lexicon-based Approach: 0.8004012036108324\n"
     ]
    }
   ],
   "source": [
    "# Download the VADER lexicon\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "# Define a function to assign sentiment labels based on the 'Score' column\n",
    "def assign_sentiment(score):\n",
    "    \n",
    "    if score >= 4:\n",
    "        return 'Positive'\n",
    "    elif score <= 2:\n",
    "        return 'Negative'\n",
    "    else:\n",
    "        return 'Neutral'\n",
    "\n",
    "# Assign sentiment labels based on the 'Score' column\n",
    "data['Sentiment'] = data['Score'].apply(assign_sentiment)\n",
    "\n",
    "# Initialize the sentiment analyzer\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Calculate sentiment scores for each review\n",
    "data['Lexicon_Sentiment'] = data['Preprocessed_Text'].apply(lambda x: sid.polarity_scores(x)['compound'])\n",
    "\n",
    "# Map sentiment scores to labels\n",
    "data['Lexicon_Sentiment_Label'] = data['Lexicon_Sentiment'].apply(lambda x: 'Positive' if x > 0 else ('Negative' if x < 0 else 'Neutral'))\n",
    "\n",
    "# Evaluate the lexicon-based approach\n",
    "lexicon_accuracy = accuracy_score(data['Sentiment'], data['Lexicon_Sentiment_Label'])\n",
    "print(\"Accuracy of the Lexicon-based Approach:\", lexicon_accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76751690",
   "metadata": {},
   "source": [
    "## 3.2 Machine-Learning based Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "98df5a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data['Preprocessed_Text'], data['Sentiment'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a TF-IDF vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the training data\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Transform the testing data\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24def13a",
   "metadata": {},
   "source": [
    "# 4.0 Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fd388142",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Classifier Accuracy: 0.77\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.00      0.00      0.00        29\n",
      "     Neutral       0.00      0.00      0.00        17\n",
      "    Positive       0.77      1.00      0.87       154\n",
      "\n",
      "    accuracy                           0.77       200\n",
      "   macro avg       0.26      0.33      0.29       200\n",
      "weighted avg       0.59      0.77      0.67       200\n",
      "\n",
      "Support Vector Machine (SVM) Classifier Accuracy: 0.795\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.70      0.24      0.36        29\n",
      "     Neutral       1.00      0.06      0.11        17\n",
      "    Positive       0.80      0.98      0.88       154\n",
      "\n",
      "    accuracy                           0.80       200\n",
      "   macro avg       0.83      0.43      0.45       200\n",
      "weighted avg       0.80      0.80      0.74       200\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ayman\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\ayman\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\ayman\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\ayman\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate Naive Bayes classifier\n",
    "nb_classifier = MultinomialNB()\n",
    "nb_classifier.fit(X_train_tfidf, y_train)\n",
    "nb_predictions = nb_classifier.predict(X_test_tfidf)\n",
    "nb_accuracy = accuracy_score(y_test, nb_predictions)\n",
    "print(\"Naive Bayes Classifier Accuracy:\", nb_accuracy)\n",
    "print(classification_report(y_test, nb_predictions))\n",
    "\n",
    "# Train and evaluate SVM classifier\n",
    "svm_classifier = LinearSVC()\n",
    "svm_classifier.fit(X_train_tfidf, y_train)\n",
    "svm_predictions = svm_classifier.predict(X_test_tfidf)\n",
    "svm_accuracy = accuracy_score(y_test, svm_predictions)\n",
    "print(\"Support Vector Machine (SVM) Classifier Accuracy:\", svm_accuracy)\n",
    "print(classification_report(y_test, svm_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0646df",
   "metadata": {},
   "source": [
    "# 5.0 Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6f9205",
   "metadata": {},
   "source": [
    "### In a general sense, the strengths and weaknesses of the each model for sentiment classification are as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620302fb",
   "metadata": {},
   "source": [
    "## 5.1 Naive Bayes\n",
    "### Strengths:\n",
    "### a) Naive Bayes classifiers are straightforward, convenient and easy to use.\n",
    "### b) Only need a minimal amount of training time, particularly when working with big datasets.\n",
    "### c) Due to its independence assumption, Naive Bayes is highly suitable and adaptable to handle irrelevant features.\n",
    "\n",
    "### Weaknesses:\n",
    "### a) Complicated feature relationships and associationsÂ may be overlooked by NB classifier.\n",
    "### b) The NB classifier makes the assumption that features are independent, which may not always be the case with data from the actual world.\n",
    "### c) The NB classifier gives zero probability to a category and a feature that do not appear together in the training data, which might cause unexpected problems or issues to arise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c88c4d",
   "metadata": {},
   "source": [
    "## 5.2 SVM \n",
    "### Strengths:\n",
    "### a) SVM has a high accuracy rate, particularly in spaces with several dimensions.\n",
    "### b) SVM is efficient in high-dimensional spaces when the number of dimensions exceeds the number of samples.\n",
    "### c) SVM provide adaptability in selecting various kernel functions to represent intricate relationships.\n",
    "### Weaknesses:\n",
    "### a) SVM training may be computationally costly and intensive, particularly when dealing with massive datasets.\n",
    "### b) SVM sensitivity to noise in the data might result in overfitting if improper regularization is not done.\n",
    "### c) SVMs has low interpretability since it offers less information about the causes of predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06120885",
   "metadata": {},
   "source": [
    "## 5.3 Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e585434b",
   "metadata": {},
   "source": [
    "### a) Based on the results, SVM has a marginal superiority over Naive Bayes in terms of accuracy, with a score of 0.795 compared to Naive Bayes' score of 0.77. \n",
    "### b) The simplicity of Naive Bayes makes it more interpretable when compared with SVM.\n",
    "### c) Computationally speaking, Naive Bayes is more efficient than SVM.\n",
    "### d) In terms of robustness, Naive Bayes exhibits more adaptability to irrelevant features, whereas SVM may have difficulties in handling noise present in the data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
